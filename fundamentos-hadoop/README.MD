## Anotações do Curso - Fundamentos e Ecossistema Hadoop

### Introdução ao Hadoop:

- O Hadoop é uma plataforma robusta desenvolvida em Java, voltada para computação distribuída.
- Sua principal finalidade é lidar com grandes volumes de dados de forma eficiente e tolerante a falhas.
- Baseia-se em um modelo de cluster, distribuindo o processamento e armazenamento de dados entre os nós.

### Componentes Essenciais do Ecossistema Hadoop:

1. **HDFS (Hadoop Distributed File System):**
   - Sistema de Arquivos Distribuído do Hadoop.
   - Projetado para armazenar grandes conjuntos de dados de forma confiável e escalável.

2. **YARN (Yet Another Resource Negotiator):**
   - Gerenciador de recursos do Hadoop.
   - Permite a alocação dinâmica de recursos entre as aplicações em um cluster.

3. **MapReduce:**
   - Modelo de programação para processamento paralelo de dados em clusters de computadores.

4. **Spark:**
   - Mecanismo de processamento de dados em memória.
   - Oferece uma alternativa mais rápida ao MapReduce para certas cargas de trabalho.

5. **Pig e Hive:**
   - Linguagens de consulta de alto nível para processamento de dados no Hadoop.
   - Facilitam a escrita e execução de consultas complexas.

6. **HBase:**
   - Banco de dados NoSQL distribuído, orientado a colunas e altamente escalável.

7. **Mahout e Spark MLlib:**
   - Bibliotecas de aprendizado de máquina para análises preditivas e exploratórias em grandes conjuntos de dados.

8. **Apache Drill:**
   - Mecanismo de consulta SQL para dados semiestruturados e estruturados armazenados em diferentes formatos.

9. **Zookeeper:**
   - Serviço de coordenação distribuída para gerenciar e coordenar serviços em um cluster Hadoop.

10. **Oozie:**
    - Sistema de agendamento de fluxo de trabalho para definir e executar fluxos de trabalho complexos no Hadoop.

### Conclusão:

- O ecossistema Hadoop é composto por uma variedade de ferramentas e serviços que se complementam para oferecer uma solução abrangente para processamento, armazenamento e análise de dados em larga escala.
- Cada componente desempenha um papel específico e é essencial para a funcionalidade global do Hadoop.
- Compreender esses componentes é fundamental para aproveitar ao máximo o potencial do Hadoop na manipulação de grandes volumes de dados de maneira eficiente e confiável.